Цель проекта

Разработать REST API, который будет принимать webhook-запросы, обрабатывать их с использованием LLM (Large Language Model), генерировать ответ на основе текста запроса и отправлять результат на указанный callback URL. Проект должен поддерживать историю сообщений для сохранения контекста диалога, а также обеспечивать асинхронную обработку запросов.
Используемые технологии

    FastAPI: Современный фреймворк для создания REST API на Python.

    PostgreSQL: База данных для хранения истории сообщений (контекста диалога).

    asyncpg: Асинхронный драйвер PostgreSQL для работы с базой данных.

    Redis: Используется для хранения и управления историей сообщений.

    Celery: Фреймворк для асинхронной обработки задач. Будет использоваться для обработки запросов в фоновом режиме.

    OpenRouter: Платформа для интеграции с различными моделями LLM (например, GPT-3.5, Claude).

    Другие 

Описание функционала

    Обработка входящих запросов:

        REST API будет иметь один основной endpoint для приема webhook-запросов по методу POST.

        Запрос будет содержать два обязательных поля:

            message: текст запроса, который будет обработан моделью.

            callback_url: URL, на который необходимо отправить сгенерированный ответ.

    Интеграция с LLM:

        В запросах к API будет использоваться интеграция с одной из моделей LLM (например, GPT-3.5, Claude или любая другая модель, доступная через OpenRouter).

        Модель будет обрабатывать текст запроса и генерировать ответ.

    Обработка контекста диалога:

        Для сохранения контекста диалога будет использован Redis, где сохраняются сообщения, связанные с каждым callback_url или сессионным ID.

        Это позволит обеспечивать контекстный ответ, если запросы поступают в рамках одного диалога.

    Асинхронная обработка:

        Все операции (обработка запросов, вызовы LLM, отправка данных на callback_url) должны выполняться асинхронно, чтобы не блокировать сервер.

        Для этого используется Celery с Redis как брокером сообщений.

    Отправка ответа на callback_url:

        После получения ответа от LLM, API отправит этот ответ на callback_url, указанный в запросе.

Функциональные требования
1. Реализовать endpoint для приема webhook-запросов (POST /webhook)
2. Интегрировать любую LLM модель (например, GPT-3.5-turbo, Claude, LLaMA и т.д.), реализовать отправку и обработку запросов в формате, совместимом с OpenAI API. Как демо версию использовать бесплатные модели из сервиса openrouter.
3. Настроить отправку результатов на заданный callback URL
4. Реализовать поддержку истории сообщений для сохранения контекста диалога


Формат входных данных
{
    "message": "текст запроса",
    "callback_url": "https://example.com/callback",
}

Критерии оценки
- Качество и чистота кода
- Правильность обработки ошибок
- Соблюдение принципов SOLID

Желательно
- Использование Docker
- Добавление конфигурации через переменные окружения либо же .env
- Реализация rate limiting
- Интеграция message broker (например, RabbitMQ, Redis, Kafka) для асинхронной обработки